{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "SentimentAnalysis_RNN_LSTM_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_OtxqGBgP0G",
        "colab_type": "text"
      },
      "source": [
        "# <span style='font-size:50px;background-color:yellow;'>&#128201;</span> <font color=\"#004080\"> *Sentiment analysis on IMDB movie review dataset ::* </font>\n",
        "\n",
        "## The dataset comprises of 50,000 movie reviews taken from IMDb. The training and test data sets are uploaded in moodle. The datasets are in the form of text files. Every line in the files is a movie review. The training set and test set comprises of 25000 movie reviews each. For both training and test data sets, the first 12500 reviews are positive and the rest of the reviews are negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDBtODUtgbX_",
        "colab_type": "code",
        "outputId": "c1628e1d-9adb-483b-9d4a-0f52a04d27cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS2oTT-1iwWx",
        "colab_type": "code",
        "outputId": "c9f070c6-720e-4caa-c406-23f92472f630",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ffb79f0d-8b63-4b59-a243-524701311fb4\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ffb79f0d-8b63-4b59-a243-524701311fb4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.txt to test.txt\n",
            "Saving train.txt to train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTIVMS2V0iWJ",
        "colab_type": "code",
        "outputId": "da4a8483-3463-48ee-eb34-418a17dabd30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_data = open(\"train.txt\").read()\n",
        "train_reviews = train_data.split(\"\\n\")\n",
        "\n",
        "test_data = open(\"test.txt\").read()\n",
        "test_reviews = test_data.split(\"\\n\")\n",
        "\n",
        "print(train_reviews[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWmix2XEhECC",
        "colab_type": "text"
      },
      "source": [
        "### (a) Preprocess the dataset: Remove punctuation and < br > tag. Convert all the characters to lower case --> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE-X1BO6qmtp",
        "colab_type": "code",
        "outputId": "301a039b-4d8c-4235-d1d9-a2b38427bc99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from string import punctuation\n",
        "print(punctuation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCT1DLVsqnTf",
        "colab_type": "code",
        "outputId": "c9aec35e-9dc8-4b46-80d8-cb68a4c98343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# for train data\n",
        "\n",
        "# removing forms of <br>\n",
        "for i, element in enumerate(train_reviews):\n",
        "    element = element.replace('<br>',' ')\n",
        "    element = element.replace('</br>',' ')\n",
        "    element = element.replace('<br />',' ')\n",
        "    train_reviews[i] = element\n",
        "\n",
        "# removing punctuations & lower the case\n",
        "for i, element in enumerate(train_reviews):\n",
        "  s = ''\n",
        "  for charac in element:\n",
        "    if charac not in punctuation:\n",
        "      s += charac.lower()\n",
        "  train_reviews[i] = s \n",
        "\n",
        "print(train_reviews[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efXuaErGBU7I",
        "colab_type": "code",
        "outputId": "e9eb7953-6811-42b6-aad5-bfefc08605a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# for test data\n",
        "\n",
        "# removing forms of <br>\n",
        "for i, element in enumerate(test_reviews):\n",
        "    element = element.replace('<br>',' ')\n",
        "    element = element.replace('</br>',' ')\n",
        "    element = element.replace('<br />',' ')\n",
        "    test_reviews[i] = element\n",
        "\n",
        "# removing punctuations & lower the case\n",
        "for i, element in enumerate(test_reviews):\n",
        "  s = ''\n",
        "  for charac in element:\n",
        "    if charac not in punctuation:\n",
        "      s += charac.lower()\n",
        "  test_reviews[i] = s \n",
        "\n",
        "print(test_reviews[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i went and saw this movie last night after being coaxed to by a few friends of mine ill admit that i was reluctant to see it because from what i knew of ashton kutcher he was only able to do comedy i was wrong kutcher played the character of jake fischer very well and kevin costner played ben randall with such professionalism the sign of a good movie is that it can toy with our emotions this one did exactly that the entire theater which was sold out was overcome by laughter during the first half of the movie and were moved to tears during the second half while exiting the theater i not only saw many women in tears but many full grown men as well trying desperately not to let anyone see them crying this movie was great and i suggest that you go see it before you judge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0MYneb-gP0M",
        "colab_type": "text"
      },
      "source": [
        "### (b) Create the target vector for training and test data. The first 12500 values of target vector is 1 and the rest of the values are zero. The target vector is same for training and test data -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F19DWAHbDy7S",
        "colab_type": "code",
        "outputId": "e40683c2-a2a5-4aed-dc2a-1ae8997e23e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# removing last empty review created after splitting with '\\n'\n",
        "train_reviews = train_reviews[:25000]\n",
        "test_reviews = test_reviews[:25000]\n",
        "print(len(train_reviews))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-9fndglDzNq",
        "colab_type": "code",
        "outputId": "7cb58ba3-b9d4-470e-acb1-cbf96f015d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJiCKD-BLFM7",
        "colab_type": "code",
        "outputId": "06893d2e-a36f-4a97-f26d-839d22d6a020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 5000)\n",
        "tokenizer.fit_on_texts(train_reviews)\n",
        "\n",
        "train_reviews_tokenized = tokenizer.texts_to_sequences(train_reviews)      \n",
        "\n",
        "print(train_reviews[0])\n",
        "print(train_reviews_tokenized[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt\n",
            "[321, 6, 3, 1073, 216, 8, 2083, 29, 1, 165, 58, 13, 45, 78, 41, 398, 116, 135, 13, 4884, 54, 4934, 146, 7, 1, 4935, 479, 68, 5, 254, 11, 1958, 6, 71, 2373, 5, 629, 69, 6, 4884, 1, 5, 1964, 1, 1431, 35, 67, 65, 202, 139, 63, 1191, 4884, 1, 4, 1, 217, 877, 30, 2927, 68, 4, 1, 4714, 9, 678, 2, 63, 1431, 50, 9, 208, 1, 378, 7, 59, 3, 1464, 3629, 772, 5, 3543, 184, 1, 398, 9, 1193, 29, 321, 3, 345, 359, 2957, 141, 127, 5, 27, 4, 122, 4884, 1464, 2314, 5, 321, 9, 516, 11, 104, 1441, 4, 54, 574, 101, 11, 321, 6, 233, 47, 3, 2235, 11, 8, 206]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMwPDbFqLFdF",
        "colab_type": "code",
        "outputId": "f055932a-a74d-4f54-b99b-6a78df422007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#test_tokenizer = Tokenizer(num_words = 5000)\n",
        "#tokenizer.fit_on_texts(test_reviews)\n",
        "\n",
        "test_reviews_tokenized = tokenizer.texts_to_sequences(test_reviews)      \n",
        "\n",
        "print(test_reviews[0])\n",
        "print(test_reviews_tokenized[0])\n",
        "print(type(test_reviews_tokenized))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i went and saw this movie last night after being coaxed to by a few friends of mine ill admit that i was reluctant to see it because from what i knew of ashton kutcher he was only able to do comedy i was wrong kutcher played the character of jake fischer very well and kevin costner played ben randall with such professionalism the sign of a good movie is that it can toy with our emotions this one did exactly that the entire theater which was sold out was overcome by laughter during the first half of the movie and were moved to tears during the second half while exiting the theater i not only saw many women in tears but many full grown men as well trying desperately not to let anyone see them crying this movie was great and i suggest that you go see it before you judge\n",
            "[9, 421, 2, 208, 10, 16, 226, 308, 99, 107, 5, 31, 3, 162, 338, 4, 1887, 529, 942, 11, 9, 12, 5, 65, 8, 82, 34, 47, 9, 678, 4, 26, 12, 60, 494, 5, 81, 216, 9, 12, 351, 250, 1, 106, 4, 3237, 51, 72, 2, 1797, 250, 1002, 15, 135, 1, 1862, 4, 3, 48, 16, 6, 11, 8, 67, 2999, 15, 252, 1390, 10, 27, 115, 595, 11, 1, 419, 730, 59, 12, 2935, 44, 12, 2989, 31, 2086, 302, 1, 84, 357, 4, 1, 16, 2, 64, 1610, 5, 1639, 302, 1, 328, 357, 130, 1, 730, 9, 20, 60, 208, 104, 361, 7, 1639, 17, 104, 375, 2284, 342, 13, 72, 258, 2675, 20, 5, 376, 249, 65, 92, 2540, 10, 16, 12, 83, 2, 9, 1413, 11, 21, 136, 65, 8, 153, 21, 1853]\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8LUIZlFPbvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# target vector for training, and test data\n",
        "\n",
        "a=[1]*12500\n",
        "b=[0]*12500\n",
        "train_target_vec = a + b\n",
        "test_target_vec = a + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kY1rqtLgP0O",
        "colab_type": "text"
      },
      "source": [
        "### (c) Create a validation set which is 20% of the training set -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLS-iU24QhWW",
        "colab_type": "code",
        "outputId": "421a6e75-32f6-4c9e-88d3-7bb8fe37b1dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#shuffling the training dataset and creating validation dataset\n",
        "#we tend to maintain the same ratio of class = 1 , class = 0, members in train and validation data\n",
        "\n",
        "# train_data_size = 25000\n",
        "# train_data_size_of_each_class = 25000\n",
        "# 20/100 * 12500 = 2500\n",
        "\n",
        "# shuffling class=1 data indices\n",
        "new_indices1 = np.arange(0, 12500)\n",
        "np.random.shuffle(new_indices1)\n",
        "\n",
        "#shuffling class=0 data indices\n",
        "new_indices0 = np.arange(12500, 25000)\n",
        "np.random.shuffle(new_indices0)\n",
        "\n",
        "#dividing training into classes\n",
        "#train_reviews1 = train_reviews[0:12500]\n",
        "train_reviews_tokenized1 = train_reviews_tokenized[0:12500]\n",
        "#train_reviews0 = train_reviews[12500:25000]\n",
        "train_reviews_tokenized0 = train_reviews_tokenized[12500:25000]\n",
        "\n",
        "#print(type(new_indices1))\n",
        "\n",
        "#shuffling data, each class separately\n",
        "#X_new1 = train_reviews1[new_indices1]\n",
        "train_reviews_tokenized1 = np.array(train_reviews_tokenized1)\n",
        "train_reviews_tokenized0 = np.array(train_reviews_tokenized0)\n",
        "X_new_tok1 = train_reviews_tokenized1[new_indices1]\n",
        "#X_new0 = train_reviews0[new_indices0 - 12500]\n",
        "X_new_tok0 = train_reviews_tokenized0[new_indices0 - 12500]\n",
        "\n",
        "# creating x_val and x_train\n",
        "X_val = np.array( list(X_new_tok1[0:2500]) + list(X_new_tok0[0:2500]) )\n",
        "X_train = np.array( list(X_new_tok1[2500:]) + list(X_new_tok0[2500:]) )\n",
        "Y_val = np.array( list([1]*2500) + list([0]*2500) )\n",
        "Y_train = np.array( list([1]*10000) + list([0]*10000) )\n",
        "\n",
        "X_test = np.array(test_reviews_tokenized[:])\n",
        "Y_test = np.array(test_target_vec[:])\n",
        "\n",
        "#Shuffling train, test, validation datasets independently\n",
        "new_indices = np.arange(0, X_train.shape[0])\n",
        "np.random.shuffle(new_indices)\n",
        "X_train = X_train[new_indices]\n",
        "Y_train = Y_train[new_indices]\n",
        "\n",
        "new_indices = np.arange(0, X_test.shape[0])\n",
        "np.random.shuffle(new_indices)\n",
        "X_test = X_test[new_indices]\n",
        "Y_test = Y_test[new_indices]\n",
        "\n",
        "new_indices = np.arange(0, X_val.shape[0])\n",
        "np.random.shuffle(new_indices)\n",
        "X_val = X_val[new_indices]\n",
        "Y_val = Y_val[new_indices]\n",
        "\n",
        "#printing sizes\n",
        "print(\"Final shapes -->\")\n",
        "print(\"Shape of X_train : {}\".format(np.shape(X_train)))\n",
        "print(\"Shape of Y_train : {}\".format(np.shape(Y_train)))\n",
        "print(\"Shape of X_test : {}\".format(np.shape(X_test)))\n",
        "print(\"Shape of Y_test : {}\".format(np.shape(Y_test)))\n",
        "print(\"Shape of X_val : {}\".format(np.shape(X_val)))\n",
        "print(\"Shape of Y_val : {}\".format(np.shape(Y_val)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final shapes -->\n",
            "Shape of X_train : (20000,)\n",
            "Shape of Y_train : (20000,)\n",
            "Shape of X_test : (25000,)\n",
            "Shape of Y_test : (25000,)\n",
            "Shape of X_val : (5000,)\n",
            "Shape of Y_val : (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_PM-L8lyXQr",
        "colab_type": "code",
        "outputId": "f0ab784e-b16f-4662-a402-f48443c148f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(Y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 ... 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyJIzgTVgP0R",
        "colab_type": "text"
      },
      "source": [
        "### (d) Train the following models -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApkIIur04Cav",
        "colab_type": "text"
      },
      "source": [
        "Integer encode the reviews, is done in section (c) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRFXg3eH7LyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Embedding, Conv1D, GRU, LSTM, MaxPool1D\n",
        "from tensorflow.keras.layers import RNN, SimpleRNN, Bidirectional"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldcT8vFMPQza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4JvBUQcgP0T",
        "colab_type": "text"
      },
      "source": [
        "### (e) Model 1 -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB37WSYb437d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train1 = pad_sequences(X_train, maxlen = 200)   \n",
        "X_test1 = pad_sequences(X_test, maxlen = 200)   \n",
        "X_val1 = pad_sequences(X_val, maxlen = 200)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGohYGYw44B2",
        "colab_type": "code",
        "outputId": "408f725a-cd87-4eb9-edc4-d8e766c45ceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model1 = tf.keras.Sequential([\n",
        "    Embedding(5000 ,128 , input_length = 200),\n",
        "    SimpleRNN(200, activation=\"tanh\"),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "print (model1.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 200, 128)          640000    \n",
            "_________________________________________________________________\n",
            "simple_rnn_5 (SimpleRNN)     (None, 200)               65800     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 706,001\n",
            "Trainable params: 706,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e0lSSMZ44Iv",
        "colab_type": "code",
        "outputId": "b7f48ac0-715f-495f-e02e-1d494981f073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "iter=15\n",
        "opt = tf.keras.optimizers.Adam(0.0002)\n",
        "model1.compile(loss='binary_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epoc1 = model1.fit(X_train1.astype(np.float32), Y_train, epochs = iter, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples\n",
            "Epoch 1/15\n",
            "20000/20000 - 58s - loss: 0.6808 - acc: 0.5526\n",
            "Epoch 2/15\n",
            "20000/20000 - 56s - loss: 0.4962 - acc: 0.7656\n",
            "Epoch 3/15\n",
            "20000/20000 - 56s - loss: 0.3581 - acc: 0.8509\n",
            "Epoch 4/15\n",
            "20000/20000 - 56s - loss: 0.1773 - acc: 0.9345\n",
            "Epoch 5/15\n",
            "20000/20000 - 56s - loss: 0.0802 - acc: 0.9744\n",
            "Epoch 6/15\n",
            "20000/20000 - 56s - loss: 0.0353 - acc: 0.9921\n",
            "Epoch 7/15\n",
            "20000/20000 - 56s - loss: 0.0089 - acc: 0.9989\n",
            "Epoch 8/15\n",
            "20000/20000 - 56s - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 9/15\n",
            "20000/20000 - 56s - loss: 8.7989e-04 - acc: 1.0000\n",
            "Epoch 10/15\n",
            "20000/20000 - 56s - loss: 4.7818e-04 - acc: 1.0000\n",
            "Epoch 11/15\n",
            "20000/20000 - 56s - loss: 3.0067e-04 - acc: 1.0000\n",
            "Epoch 12/15\n",
            "20000/20000 - 57s - loss: 1.9823e-04 - acc: 1.0000\n",
            "Epoch 13/15\n",
            "20000/20000 - 55s - loss: 1.3225e-04 - acc: 1.0000\n",
            "Epoch 14/15\n",
            "20000/20000 - 56s - loss: 9.0450e-05 - acc: 1.0000\n",
            "Epoch 15/15\n",
            "20000/20000 - 57s - loss: 6.2060e-05 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qysWiLcyO_PZ",
        "colab_type": "code",
        "outputId": "a1bb536c-cc5b-478f-8e3a-c7683496b729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# RNN\n",
        "lo, ac = model1.evaluate(X_train1.astype(np.float32), Y_train)\n",
        "print(\"Train Loss: {}\".format(lo))\n",
        "print(\"Train Accuracy: {}\".format(ac)) \n",
        "\n",
        "lo, ac = model1.evaluate(X_val1.astype(np.float32), Y_val)\n",
        "print(\"Validation Loss: {}\".format(lo))\n",
        "print(\"Validation Accuracy: {}\".format(ac)) \n",
        "\n",
        "lo, ac = model1.evaluate(X_test1.astype(np.float32), Y_test)\n",
        "print(\"Test Loss: {}\".format(lo))\n",
        "print(\"Test Accuracy: {}\".format(ac)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000/20000 [==============================] - 12s 582us/sample - loss: 4.8227e-05 - acc: 1.0000\n",
            "Train Loss: 4.8226801701821386e-05\n",
            "Train Accuracy: 1.0\n",
            "5000/5000 [==============================] - 3s 570us/sample - loss: 0.8579 - acc: 0.8320\n",
            "Validation Loss: 0.8579156356334686\n",
            "Validation Accuracy: 0.8320000171661377\n",
            "25000/25000 [==============================] - 14s 572us/sample - loss: 0.8692 - acc: 0.8352\n",
            "Test Loss: 0.8691968902683258\n",
            "Test Accuracy: 0.8352000117301941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R8iSy5qgP0V",
        "colab_type": "text"
      },
      "source": [
        "### (f) Model 2 -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8TLSpPXPy_c",
        "colab_type": "code",
        "outputId": "8019d353-8edd-41ef-9fe2-26799b49dcdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "    Embedding(5000 ,128 , input_length = 200),\n",
        "    LSTM(200, activation=\"tanh\"),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "print (model2.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 200, 128)          640000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 200)               263200    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 903,401\n",
            "Trainable params: 903,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOSJkzn-PzG3",
        "colab_type": "code",
        "outputId": "a935fcde-15ec-469e-dc21-5d22d92332d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "iter=15\n",
        "opt = tf.keras.optimizers.Adam(0.0002)\n",
        "model2.compile(loss='binary_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epoc2 = model2.fit(X_train1.astype(np.float32), Y_train, epochs = iter, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples\n",
            "Epoch 1/15\n",
            "20000/20000 - 224s - loss: 0.4630 - acc: 0.7831\n",
            "Epoch 2/15\n",
            "20000/20000 - 226s - loss: 0.2695 - acc: 0.8933\n",
            "Epoch 3/15\n",
            "20000/20000 - 227s - loss: 0.2207 - acc: 0.9151\n",
            "Epoch 4/15\n",
            "20000/20000 - 228s - loss: 0.1959 - acc: 0.9280\n",
            "Epoch 5/15\n",
            "20000/20000 - 226s - loss: 0.1732 - acc: 0.9348\n",
            "Epoch 6/15\n",
            "20000/20000 - 225s - loss: 0.1590 - acc: 0.9438\n",
            "Epoch 7/15\n",
            "20000/20000 - 223s - loss: 0.1361 - acc: 0.9513\n",
            "Epoch 8/15\n",
            "20000/20000 - 221s - loss: 0.1238 - acc: 0.9567\n",
            "Epoch 9/15\n",
            "20000/20000 - 222s - loss: 0.1097 - acc: 0.9617\n",
            "Epoch 10/15\n",
            "20000/20000 - 224s - loss: 0.0915 - acc: 0.9712\n",
            "Epoch 11/15\n",
            "20000/20000 - 222s - loss: 0.0768 - acc: 0.9759\n",
            "Epoch 12/15\n",
            "20000/20000 - 222s - loss: 0.0666 - acc: 0.9801\n",
            "Epoch 13/15\n",
            "20000/20000 - 220s - loss: 0.0733 - acc: 0.9749\n",
            "Epoch 14/15\n",
            "20000/20000 - 222s - loss: 0.0486 - acc: 0.9850\n",
            "Epoch 15/15\n",
            "20000/20000 - 220s - loss: 0.0497 - acc: 0.9855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Zdv4-0PzNW",
        "colab_type": "code",
        "outputId": "a397b8b8-e814-4651-a73f-12025a2cd8d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# LSTM\n",
        "lo, ac = model2.evaluate(X_train1.astype(np.float32), Y_train)\n",
        "print(\"Train Loss: {}\".format(lo))\n",
        "print(\"Train Accuracy: {}\".format(ac)) \n",
        "\n",
        "lo, ac = model2.evaluate(X_val1.astype(np.float32), Y_val)\n",
        "print(\"Validation Loss: {}\".format(lo))\n",
        "print(\"Validation Accuracy: {}\".format(ac)) \n",
        "\n",
        "lo, ac = model2.evaluate(X_test1.astype(np.float32), Y_test)\n",
        "print(\"Test Loss: {}\".format(lo))\n",
        "print(\"Test Accuracy: {}\".format(ac)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000/20000 [==============================] - 58s 3ms/sample - loss: 0.0310 - acc: 0.9937\n",
            "Train Loss: 0.03098615928851068\n",
            "Train Accuracy: 0.9936500191688538\n",
            "5000/5000 [==============================] - 14s 3ms/sample - loss: 0.5896 - acc: 0.8596\n",
            "Validation Loss: 0.5895959486365319\n",
            "Validation Accuracy: 0.8596000075340271\n",
            "25000/25000 [==============================] - 72s 3ms/sample - loss: 0.6065 - acc: 0.8484\n",
            "Test Loss: 0.6065459573078156\n",
            "Test Accuracy: 0.848360002040863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBNGDybNgP0X",
        "colab_type": "text"
      },
      "source": [
        "### (g) Model 3 -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P51S_1KhfX-C",
        "colab_type": "code",
        "outputId": "5d7dd0b8-d48f-49ac-abbd-ecb97587ede3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model3 = tf.keras.Sequential([\n",
        "    Embedding(5000 ,128 , input_length = 200),\n",
        "    GRU(200, activation=\"relu\"),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "print (model3.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 200, 128)          640000    \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 200)               197400    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 837,601\n",
            "Trainable params: 837,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lE3-mqvfYP9",
        "colab_type": "code",
        "outputId": "df969084-1bd7-4897-bde5-42a3ea73c6f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "iter=15\n",
        "opt = tf.keras.optimizers.Adam(0.0002)\n",
        "model3.compile(loss='binary_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epoc3 = model3.fit(X_train1.astype(np.float32), Y_train, epochs = iter, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples\n",
            "Epoch 1/15\n",
            "20000/20000 - 194s - loss: 641581822.5923 - acc: 0.6148\n",
            "Epoch 2/15\n",
            "20000/20000 - 192s - loss: 0.5407 - acc: 0.7416\n",
            "Epoch 3/15\n",
            "20000/20000 - 194s - loss: 35562.9588 - acc: 0.7821\n",
            "Epoch 4/15\n",
            "20000/20000 - 193s - loss: 0.5065 - acc: 0.7978\n",
            "Epoch 5/15\n",
            "20000/20000 - 194s - loss: 0.4832 - acc: 0.8067\n",
            "Epoch 6/15\n",
            "20000/20000 - 194s - loss: 0.4627 - acc: 0.8127\n",
            "Epoch 7/15\n",
            "20000/20000 - 194s - loss: 0.4444 - acc: 0.8184\n",
            "Epoch 8/15\n",
            "20000/20000 - 195s - loss: 0.4281 - acc: 0.8235\n",
            "Epoch 9/15\n",
            "20000/20000 - 196s - loss: 0.4137 - acc: 0.8272\n",
            "Epoch 10/15\n",
            "20000/20000 - 197s - loss: 0.4010 - acc: 0.8333\n",
            "Epoch 11/15\n",
            "20000/20000 - 196s - loss: 0.3898 - acc: 0.8377\n",
            "Epoch 12/15\n",
            "20000/20000 - 196s - loss: 0.3798 - acc: 0.8405\n",
            "Epoch 13/15\n",
            "20000/20000 - 195s - loss: 0.3708 - acc: 0.8446\n",
            "Epoch 14/15\n",
            "20000/20000 - 196s - loss: 0.3626 - acc: 0.8479\n",
            "Epoch 15/15\n",
            "20000/20000 - 197s - loss: 0.3551 - acc: 0.8508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gky6K9INfYkC",
        "colab_type": "code",
        "outputId": "c6ea62d8-3ee6-4792-e46d-fdec1a0202f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# GRU\n",
        "lo, ac = model3.evaluate(X_train1.astype(np.float32), Y_train)\n",
        "print(\"Train Loss: {}\".format(lo))\n",
        "print(\"Train Accuracy: {}\".format(ac)) \n",
        "\n",
        "lo, ac = model3.evaluate(X_val1.astype(np.float32), Y_val)\n",
        "print(\"Validation Loss: {}\".format(lo))\n",
        "print(\"Validation Accuracy: {}\".format(ac)) \n",
        "\n",
        "lo, ac = model3.evaluate(X_test1.astype(np.float32), Y_test)\n",
        "print(\"Test Loss: {}\".format(lo))\n",
        "print(\"Test Accuracy: {}\".format(ac)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000/20000 [==============================] - 47s 2ms/sample - loss: 0.3444 - acc: 0.8588\n",
            "Train Loss: 0.3443882711172104\n",
            "Train Accuracy: 0.8587999939918518\n",
            "5000/5000 [==============================] - 12s 2ms/sample - loss: 0.9301 - acc: 0.7506\n",
            "Validation Loss: 0.9300792888641357\n",
            "Validation Accuracy: 0.7505999803543091\n",
            "25000/25000 [==============================] - 59s 2ms/sample - loss: 0.6468 - acc: 0.7612\n",
            "Test Loss: 0.646802168560028\n",
            "Test Accuracy: 0.7611600160598755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi0eAYN_gP0Z",
        "colab_type": "text"
      },
      "source": [
        "### (h) Which is the best model among model1 to model3 - RNN, LSTM or GRU? Why? -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWvOQ6k-sELh",
        "colab_type": "text"
      },
      "source": [
        "LSTM is the best model as it gives the highest test and validation accuracy. Moreover it has maximum no. of parameters, so in a way has the most complex network which implies more complex learning under the given set of hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAxhh5iQgP0b",
        "colab_type": "text"
      },
      "source": [
        "### (i) Model 4 -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sj5kNLGstcP",
        "colab_type": "code",
        "outputId": "c2be9673-0a69-4c45-d8f6-bf7f36668e60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "#We used LSTM model as it worked the best.\n",
        "\n",
        "model4 = tf.keras.Sequential([\n",
        "    Embedding(5000 ,128 , input_length = 200),\n",
        "    LSTM(200, activation=\"tanh\", return_sequences=True),   #important\n",
        "    LSTM(200, activation=\"tanh\"),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "print (model4.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 200, 128)          640000    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 200, 200)          263200    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 200)               320800    \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 1,224,201\n",
            "Trainable params: 1,224,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEuHr-ApstZW",
        "colab_type": "code",
        "outputId": "9cbb1a31-ce3e-4743-9954-998a01ec86f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "iter=15\n",
        "opt = tf.keras.optimizers.Adam(0.0002)\n",
        "model4.compile(loss='binary_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epoc4 = model4.fit(X_train1.astype(np.float32), Y_train, epochs = iter, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples\n",
            "Epoch 1/15\n",
            "20000/20000 - 447s - loss: 0.4234 - acc: 0.7937\n",
            "Epoch 2/15\n",
            "20000/20000 - 446s - loss: 0.2669 - acc: 0.8935\n",
            "Epoch 3/15\n",
            "20000/20000 - 439s - loss: 0.2255 - acc: 0.9121\n",
            "Epoch 4/15\n",
            "20000/20000 - 447s - loss: 0.1949 - acc: 0.9251\n",
            "Epoch 5/15\n",
            "20000/20000 - 449s - loss: 0.1753 - acc: 0.9342\n",
            "Epoch 6/15\n",
            "20000/20000 - 448s - loss: 0.1491 - acc: 0.9456\n",
            "Epoch 7/15\n",
            "20000/20000 - 447s - loss: 0.1246 - acc: 0.9565\n",
            "Epoch 8/15\n",
            "20000/20000 - 453s - loss: 0.1000 - acc: 0.9664\n",
            "Epoch 9/15\n",
            "20000/20000 - 452s - loss: 0.0760 - acc: 0.9755\n",
            "Epoch 10/15\n",
            "20000/20000 - 444s - loss: 0.0635 - acc: 0.9801\n",
            "Epoch 11/15\n",
            "20000/20000 - 444s - loss: 0.0513 - acc: 0.9846\n",
            "Epoch 12/15\n",
            "20000/20000 - 448s - loss: 0.0389 - acc: 0.9876\n",
            "Epoch 13/15\n",
            "20000/20000 - 455s - loss: 0.0427 - acc: 0.9858\n",
            "Epoch 14/15\n",
            "20000/20000 - 441s - loss: 0.0318 - acc: 0.9895\n",
            "Epoch 15/15\n",
            "20000/20000 - 436s - loss: 0.0196 - acc: 0.9952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7A809ZzstTc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "c4db8a2d-0857-4250-dfc5-1012c94c0976"
      },
      "source": [
        "lo, ac = model4.evaluate(X_train1.astype(np.float32), Y_train)\n",
        "print(\"Train Loss: {}\".format(lo))\n",
        "print(\"Train Accuracy: {}\".format(ac)) \n",
        "\n",
        "lo, ac = model4.evaluate(X_val1.astype(np.float3|2), Y_val)\n",
        "print(\"Validation Loss: {}\".format(lo))\n",
        "print(\"Validation Accuracy: {}\".format(ac)) \n",
        "\n",
        "lo, ac = model4.evaluate(X_test1.astype(np.float32), Y_test)\n",
        "print(\"Test Loss: {}\".format(lo))\n",
        "print(\"Test Accuracy: {}\".format(ac)) "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-85f80dfda151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat3\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model4' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE_VDLwHgP0c",
        "colab_type": "text"
      },
      "source": [
        "### (j) Model 5 -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shqq4gi0B6Ey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We used LSTM model as it worked the best.\n",
        "\n",
        "model5 = tf.keras.Sequential([\n",
        "    Embedding(5000 ,128 , input_length = 200),\n",
        "    LSTM(200, activation=\"tanh\", return_sequences=True),   #important\n",
        "    LSTM(200, activation=\"tanh\", return_sequences=True),\n",
        "    LSTM(200, activation=\"tanh\"),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "print (model5.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQYZrLzeB6Bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iter=15\n",
        "opt = tf.keras.optimizers.Adam(0.0002)\n",
        "model5.compile(loss='binary_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epoc5 = model5.fit(X_train1.astype(np.float32), Y_train, epochs = iter, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2iWXjoKB56t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lo, ac = model5.evaluate(X_train1.astype(np.float32), Y_train)\n",
        "print(\"Train Loss: {}\".format(lo))\n",
        "print(\"Train Accuracy: {}\".format(ac)) \n",
        "\n",
        "lo, ac = model5.evaluate(X_val1.astype(np.float32), Y_val)\n",
        "print(\"Validation Loss: {}\".format(lo))\n",
        "print(\"Validation Accuracy: {}\".format(ac)) \n",
        "\n",
        "lo, ac = model5.evaluate(X_test1.astype(np.float32), Y_test)\n",
        "print(\"Test Loss: {}\".format(lo))\n",
        "print(\"Test Accuracy: {}\".format(ac)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlLp5Pr7gP0e",
        "colab_type": "text"
      },
      "source": [
        "### (k) Model 6 -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay8nyGGfDjpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzflxhM4Dj9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load training data\n",
        "sentences = X_train\n",
        " \n",
        "# train word2vec model\n",
        "model6 = Word2Vec(sentences, size=128, window=5, workers=4, min_count=1)\n",
        "# summarize vocabulary size in model\n",
        "words = list(model6.wv.vocab)\n",
        "print('Vocabulary size: %d' % len(words))\n",
        " \n",
        "# save model in ASCII (word2vec) format\n",
        "filename = 'embedding_word2vec.txt'\n",
        "model6.wv.save_word2vec_format(filename, binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P7y6GlxgP0g",
        "colab_type": "text"
      },
      "source": [
        "### (l) Plot the loss vs iteration and accuracy vs iteration curve for training data for all the models -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zijl0GM2C_CS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotModel(i, ep):\n",
        "  accuracy_lst = ep.history['acc']\n",
        "  loss_lst = ep.history['loss']\n",
        "\n",
        "  print(\"////////////////////////////////////////// Model {} //////////////////////////////////////////\".format(i))\n",
        "\n",
        "  plt.subplot(121)\n",
        "  plt.plot(accuracy_lst, c='g')\n",
        "  plt.xlabel('#iterations')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title(\"Accuracy Plot - Model {} \".format(i))\n",
        "\n",
        "  plt.subplot(122)    \n",
        "  plt.plot(loss_lst, c='r')\n",
        "  plt.xlabel('#iterations')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title(\"Loss Plot - Model {} \".format(i))\n",
        "\n",
        "  plt.subplots_adjust(right=1.7)\n",
        "  plt.show()\n",
        "\n",
        "  print(\"//////////////////////////////////////////////////////////////////////////////////////////////\")\n",
        "\n",
        "E = [epoc1,epoc2,epoc3,epoc4,epoc5,epoc6]\n",
        "for i in range(1,7):\n",
        "  plotModel(i, E[i-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdwVvcWqgP0i",
        "colab_type": "text"
      },
      "source": [
        "### (m) Tabulate the training, testing and validation accuracy for all the models -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKaWr3nxlt9F",
        "colab_type": "text"
      },
      "source": [
        "///////////////|//////////////// ACCURACY /////////////////////////<br>\n",
        "|- Model ---|--- Training ---|------ Test -----|--- Validation -|<br>\n",
        "|//////////////|//////////////////////////////////////////////////////////|<br>\n",
        "|------ 1 ------|--- 100.0 % ---|--- 83.52 % ---|---- 83.20% ----|<br>\n",
        "|------ 2 ------|--- 99.37 % ---|--- 84.84 % ---|---- 85.96% ----|<br>\n",
        "|------ 3 ------|--- 85.88 % ---|--- 76.12 % ---|---- 75.06% ----|<br>\n",
        "|------ 4 ------|--- 99.52 % ---|--- 00.00 % ---|---- 00.00% ----|<br>\n",
        "|------ 5 ------|--- 00.00 % ---|--- 00.00 % ---|---- 00.00% ----|<br>\n",
        "|------ 6 ------|--- 00.00 % ---|--- 00.00 % ---|---- 00.00% ----|<br>\n",
        "////////////////////////////////////////////////////////////////////////////"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cpd67tnmFsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}